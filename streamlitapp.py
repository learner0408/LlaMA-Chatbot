# -*- coding: utf-8 -*-
"""streamlitApp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1muQ9Nsg6bTNC-4FjdpT8W4uxT2aZyNa-
"""

import streamlit as st
# import torch
# import pickle
# from transformers import (
#     AutoModelForCausalLM,
#     AutoTokenizer,
#     BitsAndBytesConfig,
#     HfArgumentParser,
#     TrainingArguments,
#     pipeline,
#     logging,
# )
# from peft import LoraConfig, PeftModel
# import numpy as np

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# Fine-tuned model name
new_model = "llama-2-7b-miniguanaco"

# Load the entire model on the GPU 0
device_map = {"": 0}

# model = pickle.load(open('/content/data.pkl','rb'))

# model = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/llama-2-7b-miniguanaco',peft_config)

# Reload tokenizer to save it
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "right"

# Setting Title
st.title("LlaMA Chatbot")

# Sidebar
st.write("Welcome to the Chatbot! One place solution for all your queries! Satisfy your hunger for information by asking the right questions :)")

with st.form('my_form'):
    text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')
    submitted = st.form_submit_button('Submit')

    if submitted:
        st.info("It's working!")

def generateOutput(prompt):
  pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
  result = pipe(f"<s>[INST] {prompt} [/INST]")
  print(result[0]['generated_text'])
